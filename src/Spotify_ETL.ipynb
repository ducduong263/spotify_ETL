{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c4ffa419-5495-4d33-b2b2-f37d8120e70e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting pymysql\n  Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\nInstalling collected packages: pymysql\nSuccessfully installed pymysql-1.1.1\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3b41e33-df99-4b82-bf74-eb0c9ec5e857",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./secrets_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "51a79ab9-c31f-45af-828e-d9ad7940bb50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## LOAD AND CLEAN DATA\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad20334f-19ae-4cca-9716-f67b2843d50e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+--------+---------+----------+-----------+----------+------------+----------+-------+-------+\n|spotify_track_uri| ts|platform|ms_played|track_name|artist_name|album_name|reason_start|reason_end|shuffle|skipped|\n+-----------------+---+--------+---------+----------+-----------+----------+------------+----------+-------+-------+\n|                0|  0|       0|        0|         0|          0|         0|         143|       117|      0|      0|\n+-----------------+---+--------+---------+----------+-----------+----------+------------+----------+-------+-------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, to_timestamp, year, month, dayofmonth, dayofweek, hour, date_format, expr, when, count, sum, avg, date_trunc, round, row_number, lit\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import StringType, ArrayType, StructType, StructField\n",
    "import requests\n",
    "import base64\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import concurrent.futures\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import os\n",
    "# import pymysql\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\") \\\n",
    "    .option(\"quote\", \"\\\"\") \\\n",
    "    .option(\"escape\", \"\\\"\") \\\n",
    "    .csv(\"/FileStore/tables/spotify_history.csv\")\n",
    "    \n",
    "null_counts = df.select([sum(col(c).isNull().cast(\"int\")).alias(c) for c in df.columns])\n",
    "null_counts.show()\n",
    "\n",
    "df = df.na.fill(\"unknown\", [\"reason_start\", \"reason_end\"])\n",
    "df = df.filter(col(\"ms_played\") >= 500)\n",
    "df_cleaned = df.dropDuplicates()\n",
    "\n",
    "# Convert data types\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"ts\", to_timestamp(\"ts\", \"yyyy-MM-dd HH:mm:ss\")) \\\n",
    "    .withColumn(\"ms_played\", col(\"ms_played\").cast(\"integer\")) \\\n",
    "    .withColumn(\"shuffle\", col(\"shuffle\").cast(\"boolean\")) \\\n",
    "    .withColumn(\"skipped\", col(\"skipped\").cast(\"boolean\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6910b16e-30b4-4063-a80b-4c9563f2d6de",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CREATE DIMENSION TABLES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9a70bb76-a476-4c68-b4fc-fc8cd76e3641",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# DIM_TIME\n",
    "window_time = Window.orderBy(\"ts\")\n",
    "dim_time = df_cleaned.select(\"ts\").distinct() \\\n",
    "    .withColumn(\"time_id\", row_number().over(window_time)) \\\n",
    "    .withColumn(\"date_full\", date_trunc(\"day\", col(\"ts\"))) \\\n",
    "    .withColumn(\"day_of_week\", date_format(\"ts\", \"E\")) \\\n",
    "    .withColumn(\"hour\", hour(\"ts\"))\n",
    "\n",
    "# DIM_ARTIST\n",
    "window_artist = Window.orderBy(\"artist_name\")\n",
    "dim_artist = df_cleaned.select(\"artist_name\").distinct() \\\n",
    "    .withColumn(\"artist_id\", row_number().over(window_artist))\n",
    "\n",
    "# DIM_ALBUM\n",
    "dim_album_raw = df_cleaned.select(\"album_name\", \"artist_name\").distinct()\n",
    "dim_album = dim_album_raw.join(dim_artist, on=\"artist_name\", how=\"inner\")\n",
    "window_album = Window.orderBy(\"album_name\", \"artist_id\")\n",
    "dim_album = dim_album.withColumn(\"album_id\", row_number().over(window_album)) \\\n",
    "    .select(\"album_id\", \"album_name\", \"artist_id\")\n",
    "\n",
    "# DIM_TRACK \n",
    "window_spec = Window.partitionBy(\"spotify_track_uri\").orderBy(col(\"ts\").desc())\n",
    "ranked_tracks = df_cleaned.select(\"spotify_track_uri\", \"track_name\", \"ts\") \\\n",
    "    .withColumn(\"rank\", row_number().over(window_spec))\n",
    "dim_track = ranked_tracks.filter(col(\"rank\") == 1) \\\n",
    "    .drop(\"rank\", \"ts\") \\\n",
    "    .withColumnRenamed(\"spotify_track_uri\", \"track_id\") \\\n",
    "\n",
    "# DIM_PLATFORM\n",
    "window_platform = Window.orderBy(\"platform\")\n",
    "dim_platform = df_cleaned.select(\"platform\").distinct() \\\n",
    "    .withColumn(\"platform_id\", row_number().over(window_platform))\n",
    "\n",
    "# DIM_REASON\n",
    "window_reason = Window.orderBy(\"reason_start\", \"reason_end\")\n",
    "dim_reason = df_cleaned.select(\"reason_start\", \"reason_end\").distinct() \\\n",
    "    .withColumn(\"reason_id\", row_number().over(window_reason))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1db0911c-5a07-450c-9f6c-f694c2304bc8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## CREATE FACT TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87371acc-7fdf-43df-8390-8d66a51986cb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "c = df_cleaned.alias(\"c\")\n",
    "t = dim_time.alias(\"t\")\n",
    "tr = dim_track.alias(\"tr\")\n",
    "a = dim_artist.alias(\"a\")\n",
    "al = dim_album.alias(\"al\")\n",
    "p = dim_platform.alias(\"p\")\n",
    "r = dim_reason.alias(\"r\")\n",
    "\n",
    "fact_streams = c \\\n",
    "    .join(t, col(\"c.ts\") == col(\"t.ts\"), \"inner\") \\\n",
    "    .join(tr, col(\"c.spotify_track_uri\") == col(\"tr.track_id\"), \"inner\") \\\n",
    "    .join(a, col(\"c.artist_name\") == col(\"a.artist_name\"), \"inner\") \\\n",
    "    .join(al, (col(\"c.album_name\") == col(\"al.album_name\")) & \n",
    "               (col(\"a.artist_id\") == col(\"al.artist_id\")), \"inner\") \\\n",
    "    .join(p, col(\"c.platform\") == col(\"p.platform\"), \"inner\") \\\n",
    "    .join(r, (col(\"c.reason_start\") == col(\"r.reason_start\")) & \n",
    "              (col(\"c.reason_end\") == col(\"r.reason_end\")), \"inner\") \\\n",
    "    .select(\n",
    "        col(\"t.time_id\"),\n",
    "        col(\"tr.track_id\"),\n",
    "        col(\"a.artist_id\"),\n",
    "        col(\"al.album_id\"),\n",
    "        col(\"p.platform_id\"),\n",
    "        col(\"r.reason_id\"),\n",
    "        col(\"c.ms_played\"),\n",
    "        col(\"c.shuffle\"),\n",
    "        col(\"c.skipped\")\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b351e92-17b8-4e0a-9144-e2bdb3b632d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ADD TRACK IMAGE URLS FROM SPOTIFY API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "40245124-be8e-40ce-aeee-03cb26a3f944",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 02:50:04,613 - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "# SPOTIFY_CREDENTIALS = [\n",
    "#     {\n",
    "#         \"CLIENT_ID\": \"\",\n",
    "#         \"CLIENT_SECRET\": \"\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"CLIENT_ID\": \"\",\n",
    "#         \"CLIENT_SECRET\": \"\"\n",
    "#     },\n",
    "#     {\n",
    "#         \"CLIENT_ID\": \"\",\n",
    "#         \"CLIENT_SECRET\": \"\"\n",
    "#     }\n",
    "# ]\n",
    "\n",
    "token_cache = {}\n",
    "\n",
    "def get_spotify_token(cred_index=0):\n",
    "    current_time = int(time.time())\n",
    "    \n",
    "    if cred_index in token_cache and current_time < token_cache[cred_index][\"expires_at\"]:\n",
    "        return token_cache[cred_index][\"token\"]\n",
    "    \n",
    "    credentials = SPOTIFY_CREDENTIALS[cred_index % len(SPOTIFY_CREDENTIALS)]\n",
    "    auth_string = f\"{credentials['CLIENT_ID']}:{credentials['CLIENT_SECRET']}\"\n",
    "    auth_base64 = base64.b64encode(auth_string.encode()).decode()\n",
    "\n",
    "    try:\n",
    "        with requests.Session() as session:\n",
    "            response = session.post(\n",
    "                \"https://accounts.spotify.com/api/token\",\n",
    "                headers={\n",
    "                    \"Authorization\": f\"Basic {auth_base64}\",\n",
    "                    \"Content-Type\": \"application/x-www-form-urlencoded\"\n",
    "                },\n",
    "                data={\"grant_type\": \"client_credentials\"},\n",
    "                timeout=5\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            token_json = response.json()\n",
    "            \n",
    "            token_cache[cred_index] = {\n",
    "                \"token\": token_json[\"access_token\"],\n",
    "                \"expires_at\": current_time + token_json.get(\"expires_in\", 3600) - 60\n",
    "            }\n",
    "            return token_cache[cred_index][\"token\"]\n",
    "    except Exception as e:\n",
    "        logger.error(f\"API #{cred_index}: Token error: {e}\")\n",
    "        raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac3a106b-5edd-417e-9913-a7cbcf079c0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 02:50:04,816 - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def get_track_images_parallel(track_ids, batch_size=100, max_workers=6):\n",
    "    batches = [\n",
    "        (track_ids[i:i+batch_size], i % len(SPOTIFY_CREDENTIALS))\n",
    "        for i in range(0, len(track_ids), batch_size)\n",
    "    ]\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {\n",
    "            executor.submit(process_track_batch, batch): batch \n",
    "            for batch in batches\n",
    "        }\n",
    "        results = []\n",
    "        \n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            try:\n",
    "                results.extend(future.result())\n",
    "            except Exception as e:\n",
    "                batch = futures[future]\n",
    "                logger.warning(f\"Failed batch: {str(e)[:100]}\")\n",
    "                results.extend([(tid, None) for tid in batch[0]])\n",
    "    \n",
    "    # result_dict = {}\n",
    "    # for tid, url in results:\n",
    "    #     if url:\n",
    "    #         result_dict[tid] = url\n",
    "    # return result_dict\n",
    "    \n",
    "    return {tid: url for tid, url in results if url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2017c4e9-0d80-48c5-a52e-d6cbd065ceba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 02:50:04,919 - INFO - Received command c on object id p0\n"
     ]
    }
   ],
   "source": [
    "def process_track_batch(batch_data):\n",
    "    track_ids, cred_index = batch_data\n",
    "    max_retries = 2 \n",
    "    retry_count = 0\n",
    "    \n",
    "    while retry_count < max_retries:\n",
    "        try:\n",
    "            token = get_spotify_token(cred_index)\n",
    "            cleaned_ids = [tid.split(\":\")[-1] for tid in track_ids]\n",
    "            \n",
    "            with requests.Session() as session:\n",
    "                response = session.get(\n",
    "                    f\"https://api.spotify.com/v1/tracks?ids={','.join(cleaned_ids)}\",\n",
    "                    headers={\"Authorization\": f\"Bearer {token}\"},\n",
    "                    timeout=10\n",
    "                )\n",
    "                \n",
    "                if response.status_code == 429:\n",
    "                    retry_after = int(response.headers.get(\"Retry-After\", 2))\n",
    "                    time.sleep(retry_after)\n",
    "                    token_cache.pop(cred_index, None)\n",
    "                    retry_count += 1\n",
    "                    continue\n",
    "                    \n",
    "                if response.status_code == 401:\n",
    "                    token_cache.pop(cred_index, None)\n",
    "                    retry_count += 1\n",
    "                    continue\n",
    "                    \n",
    "                response.raise_for_status()\n",
    "                tracks = response.json().get(\"tracks\", [])\n",
    "                \n",
    "                return [\n",
    "                    (\n",
    "                        track_ids[i],\n",
    "                        max(t.get(\"album\", {}).get(\"images\", []), \n",
    "                            key=lambda x: x.get(\"width\", 0))[\"url\"] if t and t.get(\"album\", {}).get(\"images\") else None\n",
    "                    )\n",
    "                    for i, t in enumerate(tracks)\n",
    "                ]\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.debug(f\"API #{cred_index}: Attempt {retry_count+1} failed: {str(e)[:100]}\")\n",
    "            retry_count += 1\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return [(tid, None) for tid in track_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dda4a0d0-0829-40ba-a39b-a4fd7623b595",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def update_track_images():\n",
    "    dim_track.cache()\n",
    "    \n",
    "    track_ids = [row.track_id for row in dim_track.select(\"track_id\").distinct().collect()]\n",
    "\n",
    "    image_dict = get_track_images_parallel(track_ids)\n",
    "    \n",
    "    if image_dict:\n",
    "        image_df = spark.createDataFrame(\n",
    "            list(image_dict.items()), \n",
    "            \"track_id string, image_url string\"\n",
    "        )\n",
    "        \n",
    "        return dim_track.join(\n",
    "            image_df.hint(\"broadcast\"),\n",
    "            \"track_id\",\n",
    "            \"left\"\n",
    "        ).select(\n",
    "            dim_track[\"*\"],\n",
    "            image_df[\"image_url\"]\n",
    "        )\n",
    "    return dim_track"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6dae6d36-fd35-47da-99b0-b9d4e8145569",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 02:50:05,128 - INFO - Received command c on object id p0\n2025-06-11 02:50:48,072 - INFO - Closing down clientserver connection\n2025-06-11 02:50:49,536 - INFO - Track images update completed successfully!\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    dim_track = update_track_images()\n",
    "    logger.info(\"Track images update completed successfully!\")\n",
    "    \n",
    "except Exception as e:\n",
    "    logger.error(f\"Critical error: {str(e)}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "929ea6e6-ab2d-4abe-b849-9e027fa195bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "invalid_track_ids = dim_track.filter(col(\"image_url\").isNull()).select(\"track_id\")\n",
    "dim_track = dim_track.filter(col(\"image_url\").isNotNull())\n",
    "\n",
    "fact_streams = fact_streams.join(\n",
    "    invalid_track_ids,\n",
    "    on=\"track_id\",\n",
    "    how=\"left_anti\"\n",
    ")\n",
    "\n",
    "valid_artist_ids = fact_streams.select(\"artist_id\").distinct()\n",
    "dim_artist = dim_artist.join(\n",
    "    valid_artist_ids, \n",
    "    \"artist_id\", \n",
    "    \"inner\")\n",
    "\n",
    "valid_album_ids = fact_streams.select(\"album_id\").distinct()\n",
    "dim_album = dim_album.join(\n",
    "    valid_album_ids, \n",
    "    \"album_id\", \n",
    "    \"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "000fb714-a7c4-4c59-bd35-e13ef586d7ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-22 04:47:22,559 - INFO - Received command c on object id p0\n2025-05-22 04:51:30,902 - INFO - Successfully wrote all data to Delta tables\n"
     ]
    }
   ],
   "source": [
    "dim_time.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_time\")\n",
    "dim_artist.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_artist\")\n",
    "dim_album.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_album\")\n",
    "dim_track.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_track\")\n",
    "dim_platform.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_platform\")\n",
    "dim_reason.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"dim_reason\")\n",
    "fact_streams.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"fact_streams\")\n",
    "\n",
    "logger.info(\"Successfully wrote all data to Delta tables\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08028a1f-5f3d-4573-b93c-dc9346d23bdb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Connect and write data to AWS RDS MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2e57bbc-9bbc-4ec8-86e7-8c535a98a13c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-13 04:10:14,561 - INFO - Received command c on object id p0\n2025-05-13 04:10:15,452 - INFO - Successfully connected to RDS MySQL\n2025-05-13 04:10:21,203 - INFO - Starting to write 134819 rows to dim_time\n2025-05-13 04:10:39,231 - INFO - Successfully wrote data to dim_time\n2025-05-13 04:10:43,581 - INFO - Starting to write 4068 rows to dim_artist\n2025-05-13 04:10:51,697 - INFO - Successfully wrote data to dim_artist\n2025-05-13 04:11:00,082 - INFO - Starting to write 8337 rows to dim_album\n2025-05-13 04:11:11,929 - INFO - Successfully wrote data to dim_album\n2025-05-13 04:11:25,076 - INFO - Starting to write 16268 rows to dim_track\n2025-05-13 04:11:37,753 - INFO - Successfully wrote data to dim_track\n2025-05-13 04:11:41,390 - INFO - Starting to write 6 rows to dim_platform\n2025-05-13 04:11:48,631 - INFO - Successfully wrote data to dim_platform\n2025-05-13 04:11:53,170 - INFO - Starting to write 88 rows to dim_reason\n2025-05-13 04:12:00,567 - INFO - Successfully wrote data to dim_reason\n2025-05-13 04:12:53,902 - INFO - Starting to write 140438 rows to fact_streams\n2025-05-13 04:14:02,129 - INFO - Successfully wrote data to fact_streams\n2025-05-13 04:14:02,133 - INFO - All data successfully written to AWS RDS MySQL!\n"
     ]
    }
   ],
   "source": [
    "# jdbc_url = \"\"\n",
    "# db_user = \"\"\n",
    "# db_password = \"\"\n",
    "\n",
    "def test_connection():\n",
    "    try:\n",
    "        conn = pymysql.connect(\n",
    "            host=my_host,\n",
    "            user=db_user,\n",
    "            password=db_password\n",
    "        )\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f\"CREATE DATABASE IF NOT EXISTS {database_name}\")\n",
    "        conn.close()\n",
    "        logger.info(\"Successfully connected to RDS MySQL\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to RDS MySQL: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "# Write DataFrame to MySQL\n",
    "def write_to_mysql(df, table_name, mode=\"overwrite\", batch_size=10000):\n",
    "    try:\n",
    "        if df.count() == 0:\n",
    "            logger.warning(f\"DataFrame for {table_name} is empty. Skipping.\")\n",
    "            return\n",
    "\n",
    "        logger.info(f\"Starting to write {df.count()} rows to {table_name}\")\n",
    "\n",
    "        df.write \\\n",
    "            .format(\"jdbc\") \\\n",
    "            .option(\"url\", jdbc_url) \\\n",
    "            .option(\"dbtable\", table_name) \\\n",
    "            .option(\"user\", db_user) \\\n",
    "            .option(\"password\", db_password) \\\n",
    "            .option(\"driver\", \"com.mysql.cj.jdbc.Driver\") \\\n",
    "            .option(\"batchsize\", batch_size) \\\n",
    "            .option(\"numPartitions\", min(df.rdd.getNumPartitions(), 10)) \\\n",
    "            .option(\"rewriteBatchedStatements\", \"true\") \\\n",
    "            .option(\"connectTimeout\", 30000) \\\n",
    "            .option(\"socketTimeout\", 300000) \\\n",
    "            .mode(mode) \\\n",
    "            .save()\n",
    "\n",
    "        logger.info(f\"Successfully wrote data to {table_name}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error writing to {table_name}: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "try:\n",
    "    if test_connection():\n",
    "        write_to_mysql(dim_time, \"dim_time\")\n",
    "        write_to_mysql(dim_artist, \"dim_artist\")\n",
    "        write_to_mysql(dim_album, \"dim_album\")\n",
    "        write_to_mysql(dim_track, \"dim_track\")\n",
    "        write_to_mysql(dim_platform, \"dim_platform\")\n",
    "        write_to_mysql(dim_reason, \"dim_reason\")\n",
    "\n",
    "        write_to_mysql(fact_streams, \"fact_streams\", batch_size=50000)\n",
    "\n",
    "        logger.info(\"All data successfully written to AWS RDS MySQL!\")\n",
    "    else:\n",
    "        logger.error(\"Failed to establish connection to RDS. Data not written.\")\n",
    "except Exception as e:\n",
    "    logger.error(f\"An error occurred in the MySQL data writing process: {str(e)}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1931205925235286,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Spotify_ETL",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}